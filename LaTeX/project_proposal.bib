@article{10.3389/fpsyg.2016.00183,
  author   = {Bremner, Paul  and Leonards, Ute },
  title    = {Iconic Gestures for Robot Avatars, Recognition and Integration with Speech},
  journal  = {Frontiers in Psychology},
  volume   = {7},
  year     = {2016},
  url      = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2016.00183},
  doi      = {10.3389/fpsyg.2016.00183},
  issn     = {1664-1078},
  abstract = {<p>Co-verbal gestures are an important part of human communication, improving its efficiency and efficacy for information conveyance. One possible means by which such multi-modal communication might be realized remotely is through the use of a tele-operated humanoid robot avatar. Such avatars have been previously shown to enhance social presence and operator salience. We present a motion tracking based tele-operation system for the NAO robot platform that allows direct transmission of speech and gestures produced by the operator. To assess the capabilities of this system for transmitting multi-modal communication, we have conducted a user study that investigated if robot-produced iconic gestures are comprehensible, and are integrated with speech. Robot performed gesture outcomes were compared directly to those for gestures produced by a human actor, using a within participant experimental design. We show that iconic gestures produced by a tele-operated robot are understood by participants when presented alone, almost as well as when produced by a human. More importantly, we show that gestures are integrated with speech when presented as part of a multi-modal communication equally well for human and robot performances.</p>}
}

@inproceedings{7139460,
  author    = {Bremner, Paul and Leonards, Ute},
  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Efficiency of speech and iconic gesture integration for robotic and human communicators - a direct comparison},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {1999-2006},
  keywords  = {Speech;Robot kinematics;Robot sensing systems;Humanoid robots;Joints;Timing},
  doi       = {10.1109/ICRA.2015.7139460}
}

@inproceedings{8461212,
  author    = {Vatsal, Vighnesh and Hoffman, Guy},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Design and Analysis of a Wearable Robotic Forearm},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {5489-5496},
  keywords  = {Solid modeling;Collaboration;Manipulators;Elbow;Load modeling;Prototypes},
  doi       = {10.1109/ICRA.2018.8461212}
}
@inproceedings{9889423,
  author    = {Berzuk, James M. and Young, James E.},
  booktitle = {2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
  title     = {More than words: A Framework for Describing Human-Robot Dialog Designs},
  year      = {2022},
  volume    = {},
  number    = {},
  pages     = {393-401},
  keywords  = {Vocabulary;Market research;Iterative methods;Robots;social robotics;dialog;human-robot interaction;frame-work;survey},
  doi       = {10.1109/HRI53351.2022.9889423}
}
@inproceedings{evangelista2017grounding,
  title     = {Grounding natural language instructions in industrial robotics},
  author    = {Evangelista, Daniele and Villa, W and Imperoli, Marco and Vanzo, Andrea and Iocchi, Luca and Nardi, Daniele and Pretto, Alberto and others},
  booktitle = {Proc. IEEE/RSJ IROS Workshop, Hum.-Robot Interact. Collaborative Manuf. Environ.},
  pages     = {1--6},
  year      = {2017}
}
